import os
import gdown
import numpy as np
import cv2
from keras.preprocessing.image import img_to_array
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam

# Create a directory for datasets
if not os.path.exists("datasets"):
    os.mkdir("datasets")

# Google Drive links for datasets
datasets = {
    "Corn": "https://drive.google.com/uc?id=1x_6Ksz-FdFcsmnRKnrUEQEv6hEFMiWrb",
    "Potato": "https://drive.google.com/uc?id=1CSaoJ-iONMXEBPS5J8DBLQUAHa0Ud9xM",
    "Tomato": "https://drive.google.com/uc?id=1LIo8adY1KimV0BNCwVZlqOS-Xz1frVV5"
}

# Download datasets
for name, url in datasets.items():
    zip_path = f"datasets/{name}.zip"
    if not os.path.exists(zip_path):
        print(f"Downloading {name} dataset...")
        gdown.download(url, zip_path, quiet=False)
    # Unzip the datasets
    if not os.path.exists(f"datasets/{name}"):
        os.system(f"unzip {zip_path} -d datasets")

# Convert images to numpy arrays
def convert_image_to_array(image_dir):
    image = cv2.imread(image_dir)
    if image is not None:
        image = cv2.resize(image, (256, 256))
        return img_to_array(image)
    return np.array([])

# Prepare dataset
image_list, label_list = [], []
categories = {"Corn": 0, "Potato": 1, "Tomato": 2}
for category, label in categories.items():
    folder_path = f"datasets/{category}"
    for file_name in os.listdir(folder_path):
        image_path = os.path.join(folder_path, file_name)
        image_list.append(convert_image_to_array(image_path))
        label_list.append(label)

# Convert lists to numpy arrays
image_list = np.array(image_list, dtype=np.float16) / 255.0
label_list = to_categorical(np.array(label_list))

# Split dataset
x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state=10)

# Build the model
model = Sequential([
    Conv2D(32, (3, 3), activation="relu", input_shape=(256, 256, 3)),
    MaxPooling2D((3, 3)),
    Conv2D(16, (3, 3), activation="relu"),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(8, activation="relu"),
    Dense(3, activation="softmax")
])

model.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# Train the model
model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=32)

# Save the model
model.save("trained_model.h5")
print("Model saved as trained_model.h5")
