import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Google Drive dataset paths
datasets = {
    "Corn": "https://drive.google.com/uc?id=1x_6Ksz-FdFcsmnRKnrUEQEv6hEFMiWrb",
    "Potato": "https://drive.google.com/uc?id=1CSaoJ-iONMXEBPS5J8DBLQUAHa0Ud9xM",
    "Tomato": "https://drive.google.com/uc?id=1LIo8adY1KimV0BNCwVZlqOS-Xz1frVV5"
}

# Download datasets (you should unzip these manually after download)
def download_datasets():
    for name, url in datasets.items():
        zip_file = f"{name}.zip"
        if not os.path.exists(zip_file):
            print(f"Downloading {name} dataset...")
            os.system(f"gdown {url} -O {zip_file}")
        else:
            print(f"{name} dataset already exists.")

# Prepare data generators
def prepare_data_generators(base_dir):
    datagen = ImageDataGenerator(
        rescale=1.0/255.0,
        validation_split=0.2
    )
    train_gen = datagen.flow_from_directory(
        base_dir,
        target_size=(256, 256),
        batch_size=32,
        class_mode='categorical',
        subset='training'
    )
    val_gen = datagen.flow_from_directory(
        base_dir,
        target_size=(256, 256),
        batch_size=32,
        class_mode='categorical',
        subset='validation'
    )
    return train_gen, val_gen

# Define model
def create_model():
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),
        MaxPooling2D(2, 2),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(2, 2),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Main script
if __name__ == "__main__":
    download_datasets()

    # Set base directory for unzipped datasets
    base_dir = "dataset"  # Ensure the unzipped dataset is placed here
    train_gen, val_gen = prepare_data_generators(base_dir)

    # Train the model
    model = create_model()
    callbacks = [
        EarlyStopping(patience=3, restore_best_weights=True),
        ModelCheckpoint("trained_model.h5", save_best_only=True)
    ]
    model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=10,
        callbacks=callbacks
    )

    print("Model training complete. Saved as 'trained_model.h5'.")
